{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named google.protobuf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c6a0d8d3dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munpooling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munpool_layer2x2_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named google.protobuf"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unpooling import unpool_layer2x2_batch\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(labeled_path, unlabeled_path):\n",
    "\n",
    "\t# Find labeled files\n",
    "\tlabeled_files = [f for f in listdir(labeled_path) if isfile(join(labeled_path,f))]\n",
    "\t# Find unlabeled files\n",
    "\tunlabeled_files = [f for f in listdir(unlabeled_path) if isfile(join(unlabeled_path,f))]\n",
    "\n",
    "\t# Allocate array for labeled images\n",
    "\timages_labeled = np.empty([len(labeled_files), 360, 480], dtype = 'uint8')\n",
    "\n",
    "\t# Allocate array for unlabeled images\n",
    "\timages_unlabeled = np.empty([len(unlabeled_files), 360, 480, 3], dtype = 'uint8')\n",
    "\n",
    "\n",
    "\t# Reading labeled images\n",
    "\tfor n in range(0, len(labeled_files)):\n",
    "\t    images_labeled[n] = cv2.imread(join(labeled_path,labeled_files[n]), 0)\n",
    "\n",
    "\t# Reading unlabeled images\n",
    "\tfor n in range(0, len(unlabeled_files)):\n",
    "\t    images_unlabeled[n] = cv2.imread(join(unlabeled_path,unlabeled_files[n]), 1)\n",
    "\n",
    "\t# Preprocess unlabeled images\n",
    "\timages_unlabeled = images_unlabeled.astype('float64')\n",
    "\tX = imagenet_utils.preprocess_input(images_unlabeled)\n",
    "\n",
    "\t# Preprocess labeled images\n",
    "\tn_labeled = len(labeled_files)\n",
    "\tY = np_utils.to_categorical(images_labeled.flatten(), 12)\n",
    "\tY = Y.reshape((n_labeled, images_labeled.size / n_labeled, 12))\n",
    "\n",
    "\treturn X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y): \n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [3, 3, 3, 64] Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "                        W2 : [3, 3, 64, 128]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [7, 7, 3, 64], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [7, 7, 64, 64], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", [7, 7, 64, 128], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W4 = tf.get_variable(\"W4\", [7, 7, 128, 128], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W5 = tf.get_variable(\"W5\", [7, 7, 128, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W6 = tf.get_variable(\"W6\", [7, 7, 256, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W7 = tf.get_variable(\"W7\", [7, 7, 256, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W8 = tf.get_variable(\"W8\", [7, 7, 256, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W9 = tf.get_variable(\"W9\", [7, 7, 512, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W10 = tf.get_variable(\"W10\", [7, 7, 512, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    W11 = tf.get_variable(\"W11\", [7, 7, 512, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W12 = tf.get_variable(\"W12\", [7, 7, 512, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W13 = tf.get_variable(\"W13\", [7, 7, 512, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W14 = tf.get_variable(\"W14\", [7, 7, 512, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W15 = tf.get_variable(\"W15\", [7, 7, 256, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W16 = tf.get_variable(\"W16\", [7, 7, 256, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W17 = tf.get_variable(\"W17\", [7, 7, 256, 128], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W18 = tf.get_variable(\"W18\", [7, 7, 128, 128], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W19 = tf.get_variable(\"W19\", [7, 7, 128, 64], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W20 = tf.get_variable(\"W20\", [7, 7, 64, 64], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"W4\": W4,\n",
    "                  \"W5\": W5,\n",
    "                  \"W6\": W6,\n",
    "                  \"W7\": W7,\n",
    "                  \"W8\": W8,\n",
    "                  \"W9\": W9,\n",
    "                  \"W10\": W10,\n",
    "                  \"W11\": W11,\n",
    "                  \"W12\": W12,\n",
    "                  \"W13\": W13,\n",
    "                  \"W14\": W14,\n",
    "                  \"W15\": W15,\n",
    "                  \"W16\": W16,\n",
    "                  \"W17\": W17,\n",
    "                  \"W18\": W18,\n",
    "                  \"W19\": W19,\n",
    "                  \"W20\": W20}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "    W6 = parameters['W6']\n",
    "    W7 = parameters['W7']\n",
    "    W8 = parameters['W8']\n",
    "    W9 = parameters['W9']\n",
    "    W10 = parameters['W10']\n",
    "    W11 = parameters['W11']\n",
    "    W12 = parameters['W12']\n",
    "    W13 = parameters['W13']\n",
    "    W14 = parameters['W14']\n",
    "    W15 = parameters['W15']\n",
    "    W16 = parameters['W16']\n",
    "    W17 = parameters['W17']\n",
    "    W18 = parameters['W18']\n",
    "    W19 = parameters['W19']\n",
    "    W20 = parameters['W20']\n",
    "    \n",
    "    # Encoder 1\n",
    "    X = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)  \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)  \n",
    "    X, ind1 = tf.nn.max_pool_with_argmax(X, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "   \n",
    "    # Encoder 2\n",
    "    X = tf.nn.conv2d(X, W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)\n",
    "\n",
    "    X = tf.nn.conv2d(X, W4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)  \n",
    "    X, ind2 = tf.nn.max_pool_with_argmax(X, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "    # Encoder 3\n",
    "    X = tf.nn.conv2d(X, W5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W6, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W7, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)  \n",
    "    X, ind3 = tf.nn.max_pool_with_argmax(X, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    # Encoder 4\n",
    "    X = tf.nn.conv2d(X, W8, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W9, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, W10, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    X, ind4 = tf.nn.max_pool_with_argmax(X, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "    \n",
    "    # Decoder 1\n",
    "    X = unpool_layer2x2_batch(X, ind4)\n",
    "    X = tf.nn.conv2d(X, W11, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W12, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, W13, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    # Decoder 2\n",
    "    X = unpool_layer2x2_batch(X, ind3)\n",
    "    X = tf.nn.conv2d(X, W14, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W15, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W16, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    # Decoder 3\n",
    "    X = unpool_layer2x2_batch(X, ind2)\n",
    "    X = tf.nn.conv2d(X, W17, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W18, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    # Decoder 4\n",
    "    X = unpool_layer2x2_batch(X, ind1)\n",
    "    X = tf.nn.conv2d(X, W19, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.relu(X) \n",
    "    \n",
    "    X = tf.nn.conv2d(X, W20, strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.contrib.layers.batch_norm(X)\n",
    "    X = tf.nn.softmax(X) \n",
    "\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-200fa8516b0a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-200fa8516b0a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables\n",
    "    num_of_classes = Y_train.shape[1]   \n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, num_of_classes)\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_labeled_path = './CamVid/trainannot'\n",
    "train_unlabeled_path = './CamVid/train'\n",
    "test_labeled_path = './CamVid/testannot'\n",
    "test_unlabeled_path = './CamVid/test'\n",
    "\n",
    "# Load dataset\n",
    "print \"Loading training dataset\"\n",
    "X_train, Y_train = load_data(train_labeled_path, train_unlabeled_path)\n",
    "print \"Loading testing dataset\"\n",
    "X_test, Y_test = load_data(test_labeled_path, test_unlabeled_path)\n",
    "\n",
    "# Define some parameters\n",
    "input_shape = (360, 480, 3)\n",
    "minibatch_size = 1\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train the model\n",
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test, learning_rate, epochs, minibatch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
